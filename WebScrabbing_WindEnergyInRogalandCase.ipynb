{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "WebScrabbing_WindEnergyInRogalandCase.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maalde/WebScrabbingWindEnergyData_Rogaland/blob/main/WebScrabbing_WindEnergyInRogalandCase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfKMAcDgZyXl"
      },
      "source": [
        "IMPORT DEPENDENCIES"
      ],
      "id": "ZfKMAcDgZyXl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRNRebGhZyXs",
        "outputId": "ea586585-ddbb-4966-e3fe-dc6ae15a80ad"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from pathlib import Path\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install selenium\n",
        "from selenium.webdriver import Chrome\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC"
      ],
      "id": "GRNRebGhZyXs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JZrnI0JaIyl",
        "outputId": "376345bc-0f06-43b6-dd38-d7254c9d40d1"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "wd.get(\"https://www.webite-url.com\")"
      ],
      "id": "0JZrnI0JaIyl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (91.0.4472.101-0ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 70 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: use options instead of chrome_options\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZAr6JGvZyXt"
      },
      "source": [
        "Automatic scraping of Production for tomorrow at Hog-Jaeren wind plant"
      ],
      "id": "pZAr6JGvZyXt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2lFH99cZyXu"
      },
      "source": [
        "def parse(string):\n",
        "    string = string.split(':')[1]\n",
        "    string = string.replace('kWh','').replace(\",\",\"\").strip()\n",
        "    return int(string)\n",
        "ab = requests.get(\"https://www.thewindpower.net/windfarm_en_13085_hog-jaeren-energipark.php\")\n",
        "soup = BeautifulSoup(ab.text)\n",
        "bs = soup.find_all(\"li\", {\"class\": \"puce_texte\"})\n",
        "productions = [a.text for a in bs if \"expected production\" in str(a).lower()]\n",
        "exp_prod = list(map(lambda x: parse(x), productions))\n",
        "total_exp_prod = np.sum(exp_prod)"
      ],
      "id": "d2lFH99cZyXu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNxJE8MrZyXu"
      },
      "source": [
        "4 SCRAPERS (DAY AHEAD PRICE HOURLY AND DAILY, DAY AHEAD VOLUME HOURLY AND DAILY)"
      ],
      "id": "kNxJE8MrZyXu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B6KO3tjZyXv",
        "outputId": "8418471f-7d80-4b8a-f3bd-ba9803ad5ad3"
      },
      "source": [
        "i = 0\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)\n",
        "driver = wd\n",
        "driver.get(\"https://www.nordpoolgroup.com/Market-data1/Dayahead/Area-Prices/ALL1/Hourly/?view=table\")\n",
        "def click(element):\n",
        "    time.sleep(0.5)\n",
        "    i = 0\n",
        "    while i<10:\n",
        "        try:\n",
        "            element = driver.find_element_by_xpath(element)\n",
        "            element.click()\n",
        "            print('clicked')\n",
        "            break\n",
        "        except:\n",
        "            i += 1\n",
        "            print('not clicked')\n",
        "            pass\n",
        "        \n",
        "def get_prices():\n",
        "    click(\"/html/body/div[7]/div[1]/div[2]/div/img\")\n",
        "    click(\"/html/body/div[7]/div/div[2]/div/img\")\n",
        "    click(\"/html/body/div[3]/div[2]/button[2]\")\n",
        "    click(\"/html/body/div[4]/div/div/div/div[3]/div/div[3]/div[1]/div[3]/div[3]/span[2]/img\")\n",
        "    click(\"/html/body/div[4]/div/div/div/div[3]/div/div[3]/div[1]/div[3]/div[1]/div/ul/li[2]/a/span\")\n",
        "    click(\"/html/body/div[7]/div[1]/div[2]/div/img\")\n",
        "    click(\"/html/body/div[7]/div/div[2]/div/img\")\n",
        "    click(\"/html/body/div[4]/div/div/div/div[3]/div/div[3]/div[1]/div[3]/div[3]/span[2]/img\")\n",
        "\n",
        "def get_volumes():\n",
        "    click(\"/html/body/div[4]/div/div/div/div[2]/div/div/div[3]/ul/li[1]/div/div[2]/ul/li[2]/div/div/a\")\n",
        "    click(\"/html/body/div[4]/div/div/div/div[3]/div/div[2]/div/ul/li[2]/a/span\")\n",
        "    click(\"/html/body/div[7]/div/div[2]/div/img\")\n",
        "    click(\"/html/body/div[4]/div/div/div/div[3]/div/div[3]/div[1]/div[1]/div/div/ul/li[3]/a\")\n",
        "    click(\"/html/body/div[4]/div/div/div/div[3]/div/div[3]/div[1]/div[3]/div[3]/span[2]/img\")\n",
        "    click(\"/html/body/div[4]/div/div/div/div[3]/div/div[3]/div[1]/div[3]/div[1]/div/ul/li[1]/a/span\")\n",
        "    click(\"/html/body/div[7]/div/div[2]/div/img\")\n",
        "    click(\"/html/body/div[4]/div/div/div/div[3]/div/div[3]/div[1]/div[3]/div[3]/span[2]/img\")\n",
        "get_prices()\n",
        "get_volumes()\n"
      ],
      "id": "-B6KO3tjZyXv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: use options instead of chrome_options\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n",
            "clicked\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgWJVA1XZyXx"
      },
      "source": [
        "try:\n",
        "    files = os.listdir(r\"/Users/maalidefaamosestantuoyir/Desktop/Case Interview Material/files\")\n",
        "    \n",
        "    [os.remove(f\"/Users/maalidefaamosestantuoyir/Desktop/Case Interview Material/files/{file}\") for file in files]\n",
        "except:\n",
        "    print('error')\n",
        "    \n",
        "dirpath = r\"/Users/maalidefaamosestantuoyir/Downloads/\"\n",
        "paths = sorted(Path(dirpath).iterdir(), key=os.path.getmtime, reverse = True)\n",
        "for i in range(4):\n",
        "    name_dict = {0:\"DAV_h.xls\",1:\"DAV_d.xls\",2:\"DAP_d.xls\",3:\"DAP_h.xls\"}\n",
        "    name =str(paths[i]).split('\\\\')[-1]\n",
        "    os.rename(str(paths[i]), fr\"/Users/maalidefaamosestantuoyir/Desktop/Case Interview Material/files/{name_dict[i]}\")"
      ],
      "id": "SgWJVA1XZyXx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8i3Eot9ZyXy"
      },
      "source": [
        "def get_text(listed):\n",
        "    listed = list(map(lambda x: x.text.replace(u'\\xa0',''), listed))\n",
        "    listed = list(map(lambda x: x.replace(',','.'), listed))\n",
        "    return listed\n",
        "def gen_DAP_d():\n",
        "    with open('DAP_d.xl', 'rb') as file:\n",
        "        df = file.read()\n",
        "        df = df.decode()\n",
        "    print(df)\n",
        "    dates = BeautifulSoup(df).find_all('td',{'class': 'row-name'})\n",
        "    buy = BeautifulSoup(df).find_all('td',{'class': 'sortable'})\n",
        "    atts = BeautifulSoup(df).find_all('th',{'class': \"draggable sortable\"})\n",
        "    dates = get_text(dates)\n",
        "    buy = get_text(buy)\n",
        "    atts = get_text(atts)\n",
        "    cities = ['Oslo','Kr.sand', 'Bergen', 'Molde', 'Tr.heim', 'TromsÃ¸']\n",
        "    bools = np.where(np.array(list(map(lambda x: x in ['Oslo','Kr.sand', 'Bergen', 'Molde', 'Tr.heim', 'TromsÃ¸'], atts))))[0]\n",
        "    nor_array = np.array([bools + len(atts)*x for x in range(len(dates))])\n",
        "    df_norge = pd.DataFrame([np.array(buy)[nor_array][i].ravel() for i in range(len(nor_array))])\n",
        "    print(df_norge)\n",
        "\n",
        "    df_norge.columns = ['Oslo','Kr.sand', 'Bergen', 'Molde', 'Tr.heim', 'TromsÃ¸']\n",
        "    df_norge.index = dates\n",
        "    df_norge.index.name = \"date\"\n",
        "    df_norge = df_norge.applymap(float)\n",
        "    return df_norge\n",
        "def gen_DAP_h():\n",
        "    with open('DAP_h.xls') as file:\n",
        "        df = file.read()    \n",
        "    hours = BeautifulSoup(df).find_all('td',{'class': 'row-name'})\n",
        "    prices = BeautifulSoup(df).find_all('td',{'class': 'sortable'})\n",
        "    atts = BeautifulSoup(df).find_all('th',{'class': \"draggable sortable\"})\n",
        "    hours = get_text(hours)\n",
        "    hours = list(map(lambda x: x.split('-')[0],hours))\n",
        "    prices = get_text(prices)\n",
        "    atts = get_text(atts)\n",
        "    cities = ['Oslo','Kr.sand', 'Bergen', 'Molde', 'Tr.heim', 'TromsÃ¸']\n",
        "    bools = np.where(np.array(list(map(lambda x: x in ['Oslo','Kr.sand', 'Bergen', 'Molde', 'Tr.heim', 'TromsÃ¸'], atts))))[0]\n",
        "    nor_array = np.array([bools + len(atts)*x for x in range(len(hours))])\n",
        "    df_norge = pd.DataFrame([np.array(prices)[nor_array][i].ravel() for i in range(len(nor_array))])\n",
        "    print(df_norge)\n",
        "    df_norge.columns = ['Oslo','Kr.sand', 'Bergen', 'Molde', 'Tr.heim', 'TromsÃ¸']\n",
        "    df_norge.index = hours\n",
        "    df_norge.drop('', inplace = True)\n",
        "    df_norge.index.name = \"hour\"\n",
        "    df_norge = df_norge.applymap(float)\n",
        "    df_norge = df_norge.iloc[:-6,:]\n",
        "    df_norge\n",
        "    return df_norge\n",
        "def gen_DAV_d():\n",
        "    with open('DAV_d.xls') as file:\n",
        "        df = file.read()\n",
        "    dates = BeautifulSoup(df).find_all('td',{'class': 'row-name'})\n",
        "    prices = BeautifulSoup(df).find_all('td',{'class': 'sortable'})\n",
        "    cities = BeautifulSoup(df).find_all('th',{'class': \"sortable draggable\"})\n",
        "    dates = get_text(dates)\n",
        "    prices = get_text(prices)\n",
        "    sell = prices[1::2]\n",
        "    buy = prices[::2]\n",
        "    df1 = pd.DataFrame([dates,buy,sell]).transpose()\n",
        "    df1.columns = ['date', 'buy','sell']\n",
        "    df1.set_index('date', inplace = True)\n",
        "    df1 = df1.applymap(float)\n",
        "    return df1\n",
        "def gen_DAV_h():\n",
        "    with open('DAV_h.xls') as file:\n",
        "        df = file.read()\n",
        "    hours = BeautifulSoup(df).find_all('td',{'class': 'row-name'})\n",
        "    prices = BeautifulSoup(df).find_all('td',{'class': 'sortable'})\n",
        "    cities = BeautifulSoup(df).find_all('th',{'class': \"sortable draggable\"})\n",
        "    hours = get_text(hours)\n",
        "    prices = get_text(prices)\n",
        "    sell = prices[1::2]\n",
        "    buy = prices[::2]\n",
        "    hours = list(map(lambda x: x.split('-')[0],hours))\n",
        "    df1 = pd.DataFrame([hours,buy,sell]).transpose()\n",
        "    df1.columns = ['hour', 'buy','sell']\n",
        "    df1.set_index('hour', inplace = True)\n",
        "    df1 = df1.drop('')\n",
        "    df1 = df1[:-3]\n",
        "    df1 = df1.applymap(float)\n",
        "    return df1"
      ],
      "id": "O8i3Eot9ZyXy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANa9IhokZyX1"
      },
      "source": [
        "os.chdir(fr\"/Users/maalidefaamosestantuoyir/Desktop/Case Interview Material/files\")"
      ],
      "id": "ANa9IhokZyX1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf2VlsBiZyX2"
      },
      "source": [
        "gen_DAP_d().plot()"
      ],
      "id": "Zf2VlsBiZyX2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvFotk7RZyX3"
      },
      "source": [
        "gen_DAP_h().plot()"
      ],
      "id": "QvFotk7RZyX3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_udghaKZyX3"
      },
      "source": [
        "gen_DAP_h().plot()"
      ],
      "id": "b_udghaKZyX3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rZkP5_YZyX4"
      },
      "source": [
        "BUY AND SELL PLOT DAILY"
      ],
      "id": "1rZkP5_YZyX4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA7v4nkXZyX4"
      },
      "source": [
        "gen_DAV_d().plot()"
      ],
      "id": "IA7v4nkXZyX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BouMyUwLZyX4"
      },
      "source": [
        "BUY-TO-SELL RATIO PLOT"
      ],
      "id": "BouMyUwLZyX4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffXg1ZaoZyX5"
      },
      "source": [
        "dates = (gen_DAV_d()['buy']/gen_DAV_d()['sell']).index\n",
        "indices = np.arange(0,len(dates),1)\n",
        "vals = (gen_DAV_d()['buy']/gen_DAV_d()['sell'])\n",
        "plt.plot(indices,vals)\n",
        "plt.plot(indices, np.tile(1,len(indices)))"
      ],
      "id": "ffXg1ZaoZyX5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrBZi0yZZyX5"
      },
      "source": [
        "data = requests.get(\"https://www.yr.no/nb/detaljer/tabell/11-26732/Norge/Rogaland/H%C3%A5/H%C3%B8g-J%C3%A6ren\")\n",
        "soup = BeautifulSoup(data.text)\n",
        "speeds = [soup.findAll(\"td\", {\"class\": \"fluid-table__cell fluid-table__cell--align-right\"})[8*i+4].text for i in range(24)]\n",
        "speeds = list(map(lambda x: x.split('m/s')[0].strip(), speeds))\n",
        "speeds = np.array(list(map(lambda x: int(x), speeds)))\n",
        "speeds"
      ],
      "id": "GrBZi0yZZyX5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnnCusY-ZyX5"
      },
      "source": [
        "prod_per_speed = total_exp_prod/np.sum(speeds)"
      ],
      "id": "QnnCusY-ZyX5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oEBDP-UZyX5"
      },
      "source": [
        "city = \"Bergen\"\n",
        "b2s = pd.concat([gen_DAP_h()[city],gen_DAV_h()],axis = 1)\n",
        "b2s['b2s']=b2s['buy']/b2s['sell']\n",
        "b2s['prod'] = speeds*prod_per_speed\n",
        "b2s['amount_sold']= b2s['prod']*b2s['b2s']\n",
        "b2s['amount_remaining'] = b2s['prod'] - b2s['amount_sold']\n",
        "b2s['revenue'] = b2s[city]*b2s['amount_sold']\n",
        "b2s['storage']=b2s['amount_sold'].cumsum()"
      ],
      "id": "7oEBDP-UZyX5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vafaPrpYZyX6"
      },
      "source": [
        "POWER SOLD, REMAINING AND STORED (TOMORROW)"
      ],
      "id": "vafaPrpYZyX6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhbiHipSZyX6"
      },
      "source": [
        "b2s[['amount_sold','amount_remaining','storage']].plot()"
      ],
      "id": "HhbiHipSZyX6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5PQ8rDJZyX6"
      },
      "source": [
        "REVENUE HOURLY FOR THE TOMORROW"
      ],
      "id": "O5PQ8rDJZyX6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQIRGTMDZyX7"
      },
      "source": [
        "plt.scatter(b2s['revenue'].index, b2s['revenue'])"
      ],
      "id": "MQIRGTMDZyX7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-XsV4RgZyX7"
      },
      "source": [
        "total_exp_prod"
      ],
      "id": "0-XsV4RgZyX7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVdq-PcpZyX7"
      },
      "source": [
        "def to_dt(dt_obj):\n",
        "    return dt.datetime.strptime(dt_obj, \"%Y-%m-%d %H:%M:%S\")\n",
        "df = pd.read_csv(r\"/Users/maalidefaamosestantuoyir/Desktop/Produksjon per døgn [MWh].csv\")\n",
        "df[\"Dato_Id\"].map(to_dt)\n",
        "df[\"Dato_Id\"] = df[\"Dato_Id\"].map(to_dt)\n",
        "year_now = dt.datetime.now().year\n",
        "time_series = df[df[\"Dato_Id\"].map(lambda x: x.year) >= year_now - 11]\n",
        "time_series.set_index(\"Dato_Id\", inplace = True)\n",
        "time_series.plot()"
      ],
      "id": "gVdq-PcpZyX7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnhTyK9IZyX8"
      },
      "source": [
        "Filter data from 2010"
      ],
      "id": "SnhTyK9IZyX8"
    }
  ]
}